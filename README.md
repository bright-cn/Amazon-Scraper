# äºšé©¬é€Šçˆ¬è™«

[![æ¨å¹¿](https://github.com/bright-cn/Amazon-scraper/blob/main/images/Proxies%20and%20scrapers%20GitHub%20bonus%20banner.png)](https://bright.cn/products/web-scraper/amazon?promo=github15)

## ç›®å½•

- [å…è´¹äºšé©¬é€Šçˆ¬è™«](#å…è´¹äºšé©¬é€Šçˆ¬è™«)
   - [å‰ææ¡ä»¶](#å‰ææ¡ä»¶)
   - [å¿«é€Ÿè®¾ç½®](#å¿«é€Ÿè®¾ç½®)
   - [å¦‚ä½•çˆ¬å–äºšé©¬é€Šæ•°æ®](#å¦‚ä½•çˆ¬å–äºšé©¬é€Šæ•°æ®)
   - [è¾“å‡º](#è¾“å‡º)
- [çˆ¬å–äºšé©¬é€Šæ•°æ®çš„æŒ‘æˆ˜](#çˆ¬å–äºšé©¬é€Šæ•°æ®çš„æŒ‘æˆ˜)
- [è§£å†³æ–¹æ¡ˆï¼šBright CN äºšé©¬é€Šçˆ¬è™« API](#è§£å†³æ–¹æ¡ˆbright-cn-äºšé©¬é€Šçˆ¬è™«-api)
- [äºšé©¬é€Šçˆ¬è™« API å®è·µ](#äºšé©¬é€Šçˆ¬è™«-api-å®è·µ)
   - [ä½¿ç”¨ API å‚æ•°è‡ªå®šä¹‰æ•°æ®æ”¶é›†](#ä½¿ç”¨-api-å‚æ•°è‡ªå®šä¹‰æ•°æ®æ”¶é›†)
   - [äºšé©¬é€Šäº§å“æ•°æ®](#äºšé©¬é€Šäº§å“æ•°æ®)
   - [äºšé©¬é€Šè¯„è®ºæ•°æ®](#äºšé©¬é€Šè¯„è®ºæ•°æ®)
   - [äºšé©¬é€Šäº§å“æœç´¢](#äºšé©¬é€Šäº§å“æœç´¢)
   - [äºšé©¬é€Šå–å®¶ä¿¡æ¯](#äºšé©¬é€Šå–å®¶ä¿¡æ¯)
   - [æŒ‰ç•…é”€å•†å“åˆ†ç±»çš„äºšé©¬é€Šäº§å“](#äºšé©¬é€Šç•…é”€å•†å“)
   - [æŒ‰ç±»åˆ« URL åˆ†ç±»çš„äºšé©¬é€Šäº§å“](#æŒ‰ç±»åˆ«-url-çš„äºšé©¬é€Šäº§å“)
   - [æŒ‰å…³é”®è¯åˆ†ç±»çš„äºšé©¬é€Šäº§å“](#æŒ‰å…³é”®è¯çš„äºšé©¬é€Šäº§å“)
   - [å…¨çƒäºšé©¬é€Šäº§å“æ•°æ®é›†](#å…¨çƒäºšé©¬é€Šäº§å“æ•°æ®é›†)
   - [å…¨çƒäºšé©¬é€Šäº§å“æ•°æ®é›† - æŒ‰ç±»åˆ« URL å‘ç°](#å…¨çƒäºšé©¬é€Šäº§å“æ•°æ®é›†---æŒ‰ç±»åˆ«-url-å‘ç°)
   - [å…¨çƒäºšé©¬é€Šäº§å“æ•°æ®é›† - æŒ‰å…³é”®è¯å‘ç°](#å…¨çƒäºšé©¬é€Šäº§å“æ•°æ®é›†---æŒ‰å…³é”®è¯å‘ç°)

## å…è´¹äºšé©¬é€Šçˆ¬è™«

ä½¿ç”¨æ­¤å…è´¹å·¥å…·ç›´æ¥ä»æœç´¢ç»“æœé¡µé¢æå–äºšé©¬é€Šäº§å“æ•°æ®ã€‚åªéœ€å‡ ä¸ªç®€å•æ­¥éª¤å³å¯è½»æ¾è·å–äº§å“æ ‡é¢˜ã€ä»·æ ¼ã€è¯„åˆ†ã€è¯„è®ºç­‰ä¿¡æ¯ã€‚

### å‰ææ¡ä»¶

- Python 3.11 æˆ–æ›´é«˜ç‰ˆæœ¬ã€‚
- å®‰è£…å¿…è¦çš„ä¾èµ–é¡¹ï¼ˆè§ä¸‹æ–¹æ­¥éª¤ï¼‰ã€‚

### å¿«é€Ÿè®¾ç½®

1. æ‰“å¼€ç»ˆç«¯å¹¶å¯¼èˆªåˆ°æ­¤é¡¹ç›®çš„ç›®å½•ã€‚
2. è¿è¡Œä»¥ä¸‹å‘½ä»¤ä»¥å®‰è£…ä¾èµ–é¡¹ï¼š

  ```bash
    pip install -r requirements.txt
```

### å¦‚ä½•çˆ¬å–äºšé©¬é€Šæ•°æ®

è¦å¼€å§‹çˆ¬å–äºšé©¬é€Šæ•°æ®ï¼Œåªéœ€æä¾›ä¸€ä¸ªæœç´¢æŸ¥è¯¢ã€‚æ‚¨è¿˜å¯ä»¥æŒ‡å®šäºšé©¬é€Šçš„åŸŸåä»¥åŠè¦çˆ¬å–çš„é¡µé¢æ•°é‡ã€‚

#### å‘½ä»¤ï¼š
```bash
python main.py "coffee maker" --domain="com" --pages=3
```

### è¾“å‡º

çˆ¬å–å®Œæˆåï¼Œæå–çš„æ•°æ®å°†ä¿å­˜ä¸º `amazon_data.csv` æ–‡ä»¶ï¼Œä½äºé¡¹ç›®ç›®å½•ä¸­ã€‚CSV æ–‡ä»¶å°†åŒ…å«ä»¥ä¸‹è¯¦ç»†ä¿¡æ¯ï¼š
- **åç§°ï¼š** äº§å“æ ‡é¢˜ã€‚
- **å½“å‰ä»·æ ¼ï¼š** äº§å“ä»·æ ¼ï¼ˆå¦‚æœç¼ºè´§åˆ™ä¸ºç©ºï¼‰ã€‚
- **è¯„åˆ†ï¼š** å¹³å‡å®¢æˆ·è¯„åˆ†ã€‚
- **è¯„è®ºï¼š** å®¢æˆ·è¯„è®ºæ€»æ•°ã€‚
- **ASINï¼š** äºšé©¬é€Šæ ‡å‡†è¯†åˆ«å·ã€‚
- **é“¾æ¥ï¼š** äº§å“é¡µé¢çš„ç›´æ¥ URLã€‚

ä»¥ä¸‹æ˜¯æ•°æ®çš„ç¤ºä¾‹æ ¼å¼ï¼š

<img width="700" alt="bright-cn-amazon_csv_data" src="https://github.com/bright-cn/Amazon-scraper/blob/main/images/bright-data-amazon_csv_data.png">

## çˆ¬å–äºšé©¬é€Šæ•°æ®çš„æŒ‘æˆ˜

çˆ¬å–äºšé©¬é€Šæ•°æ®å¹¶éæ€»æ˜¯è½»è€Œæ˜“ä¸¾ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸è§çš„æŒ‘æˆ˜ï¼š

1. **é«˜çº§åçˆ¬æªæ–½ï¼š** äºšé©¬é€Šä½¿ç”¨ CAPTCHAã€éšå½¢æœºå™¨äººæ£€æµ‹æŠ€æœ¯ä»¥åŠè¡Œä¸ºåˆ†æï¼ˆä¾‹å¦‚è·Ÿè¸ªé¼ æ ‡ç§»åŠ¨ï¼‰æ¥é˜»æ­¢çˆ¬è™«ã€‚
2. **é¢‘ç¹çš„é¡µé¢ç»“æ„æ›´æ–°ï¼š** äºšé©¬é€Šç»å¸¸æ›´æ”¹å…¶ HTML ç»“æ„ã€ID å’Œç±»åï¼Œéœ€è¦å®šæœŸæ›´æ–°çˆ¬è™«ä»¥é€‚åº”æ–°çš„é¡µé¢å¸ƒå±€ã€‚
3. **é«˜èµ„æºæ¶ˆè€—ï¼š** ä½¿ç”¨ Playwright æˆ– Selenium ç­‰å·¥å…·çˆ¬å– JavaScript å¯†é›†å‹é¡µé¢ä¼šæ¶ˆè€—å¤§é‡ç³»ç»Ÿèµ„æºã€‚å¤„ç†åŠ¨æ€å†…å®¹å¹¶è¿è¡Œå¤šä¸ªæµè§ˆå™¨å®ä¾‹å¯èƒ½ä¼šé™ä½æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨çˆ¬å–å¤§é‡æ•°æ®æ—¶ã€‚

ä»¥ä¸‹æ˜¯äºšé©¬é€Šæ£€æµ‹åˆ°è‡ªåŠ¨çˆ¬å–å°è¯•æ—¶çš„ç¤ºä¾‹ï¼š

<img src="https://github.com/bright-cn/Amazon-scraper/blob/main/images/Amazon%20Blocked.png" alt="Amazon Blocked" width="700"/>

å¦‚ä¸Šæ‰€ç¤ºï¼Œäºšé©¬é€Šé˜»æ­¢äº†è¯¥è¯·æ±‚ä»¥é˜²æ­¢è¿›ä¸€æ­¥çš„æ•°æ®çˆ¬å–â€”â€”è¿™æ˜¯è®¸å¤šçˆ¬è™«å¸¸é‡åˆ°çš„é—®é¢˜ã€‚

## è§£å†³æ–¹æ¡ˆï¼šBright CN äºšé©¬é€Šçˆ¬è™« API

[Bright CN äºšé©¬é€Šçˆ¬è™« API](https://bright.cn/products/web-scraper/amazon) æ˜¯å¤§è§„æ¨¡çˆ¬å–äºšé©¬é€Šäº§å“æ•°æ®çš„ç»ˆæè§£å†³æ–¹æ¡ˆã€‚ä»¥ä¸‹æ˜¯å®ƒçš„ä¼˜åŠ¿ï¼š

- **æ— éœ€åŸºç¡€è®¾æ–½ç®¡ç†ï¼š** æ— éœ€å¤„ç†ä»£ç†æˆ–è§£é™¤å°é”ç³»ç»Ÿã€‚
- **åœ°ç†ä½ç½®çˆ¬å–ï¼š** å¯ä»ä»»ä½•åœ°ç†åŒºåŸŸçˆ¬å–æ•°æ®ã€‚
- **å…¨çƒ IP è¦†ç›–ï¼š** é€šè¿‡ [è¶…è¿‡ 7200 ä¸‡çœŸå®ç”¨æˆ· IP](https://bright.cn/proxy-types/residential-proxies) å’Œ [è¦†ç›– 195 ä¸ªå›½å®¶](https://bright.cn/locations)ï¼Œæä¾› 99.99% çš„æ­£å¸¸è¿è¡Œæ—¶é—´ã€‚
- **çµæ´»çš„æ•°æ®äº¤ä»˜ï¼š** å¯é€šè¿‡ Amazon S3ã€Google Cloudã€Azureã€Snowflake æˆ– SFTP ä»¥ JSONã€NDJSONã€CSV å’Œ `.gz` æ ¼å¼è·å–æ•°æ®ã€‚
- **éšç§åˆè§„ï¼š** å®Œå…¨ç¬¦åˆ GDPRã€CCPA å’Œå…¶ä»–æ•°æ®ä¿æŠ¤æ³•å¾‹ã€‚
- **å…¨å¤©å€™æ”¯æŒï¼š** ä¸“ä¸šæ”¯æŒå›¢é˜Ÿ 24/7 æä¾›å¸®åŠ©ï¼Œè§£å†³æ‰€æœ‰ä¸ API ç›¸å…³çš„é—®é¢˜ã€‚

æ­¤å¤–ï¼Œæ‚¨å¯è·å¾— **20 æ¬¡å…è´¹ API è°ƒç”¨** ä»¥æµ‹è¯•äº§å“å¹¶éªŒè¯å…¶æ˜¯å¦é€‚åˆæ‚¨çš„éœ€æ±‚ã€‚

## äºšé©¬é€Šçˆ¬è™« API å®è·µ

> æœ‰å…³è®¾ç½®äºšé©¬é€Šçˆ¬è™« API çš„è¯¦ç»†æŒ‡å—ï¼Œè¯·æŸ¥é˜…æˆ‘ä»¬çš„ [åˆ†æ­¥è®¾ç½®æŒ‡å—](https://github.com/bright-cn/Amazon-scraper/blob/main/scraper_api_setup.md#amazon-reviews)ã€‚

### ä½¿ç”¨ API å‚æ•°è‡ªå®šä¹‰æ•°æ®æ”¶é›†

é€šè¿‡ä»¥ä¸‹ API å‚æ•°è‡ªå®šä¹‰æ•°æ®æ”¶é›†æ–¹å¼ï¼š

| **å‚æ•°**           | **ç±»å‹**   | **æè¿°**                                                                                         | **ç¤ºä¾‹**                                               |
|---------------------|------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------|
| `limit`             | `integer`  | é™åˆ¶æ¯ä¸ªè¾“å…¥è¿”å›çš„ç»“æœæ•°é‡ã€‚                                                                     | `limit=10`                                            |
| `include_errors`    | `boolean`  | åœ¨è¾“å‡ºä¸­åŒ…å«é”™è¯¯æŠ¥å‘Šä»¥ä¾¿æ’æŸ¥é—®é¢˜ã€‚                                                               | `include_errors=true`                                 |
| `notify`            | `url`      | æ•°æ®æ”¶é›†å®Œæˆåå‘é€é€šçŸ¥çš„ URLã€‚                                                                   | `notify=https://notify-me.com/`                       |
| `format`            | `enum`     | æ•°æ®äº¤ä»˜çš„æ ¼å¼ã€‚æ”¯æŒçš„æ ¼å¼ï¼šJSONã€NDJSONã€JSONLã€CSVã€‚                                           | `format=json`                                         |

ğŸ’¡ **é¢å¤–äº¤ä»˜æ–¹å¼ï¼š** æ•°æ®å¯ä»¥é€šè¿‡ [Webhook](https://docs.bright.cn/scraping-automation/web-data-apis/web-scraper-api/overview#via-webhook) æˆ– [API](https://docs.bright.cn/scraping-automation/web-data-apis/web-scraper-api/overview#via-api) æä¾›ã€‚

### äºšé©¬é€Šäº§å“æ•°æ®

é€šè¿‡æä¾›äº§å“ URLï¼Œæ”¶é›†äºšé©¬é€Šçš„è¯¦ç»†äº§å“æ•°æ®ã€‚

<img width="700" alt="bright-cn-web-scraper-api-amazon-product-data" src="https://github.com/bright-cn/Amazon-scraper/blob/main/images/bright-data-web-scraper-api-amazon-product-data.png">

#### å…³é”®è¾“å…¥å‚æ•°ï¼š
| å‚æ•°       | ç±»å‹     | æè¿°                         | æ˜¯å¦å¿…éœ€ |
|------------|----------|------------------------------|----------|
| `url`      | `string` | äºšé©¬é€Šäº§å“ URLï¼Œç”¨äºçˆ¬å–æ•°æ® | æ˜¯       |

#### æ€§èƒ½ï¼š
- æ¯ä¸ªè¾“å…¥çš„å¹³å‡å“åº”æ—¶é—´ï¼š13 ç§’

#### ç¤ºä¾‹è¾“å‡ºæ•°æ®ï¼š
ä»¥ä¸‹æ˜¯çˆ¬å–äºšé©¬é€Šäº§å“æ•°æ®åç”Ÿæˆçš„è¾“å‡ºç¤ºä¾‹ï¼š
```json
{
    "url": "https://www.amazon.com/KitchenAid-Protective-Dishwasher-Stainless-8-72-Inch/dp/B07PZF3QS3",
    "title": "KitchenAid All Purpose Kitchen Shears with Protective Sheath...",
    "seller_name": "Amazon.com",
    "brand": "KitchenAid",
    "description": "These all-purpose shears from KitchenAid are a valuable addition...",
    "initial_price": 11.99,
    "final_price": 8.99,
    "currency": "USD",
    "availability": "In Stock",
    "reviews_count": 77557,
    "rating": 4.8,
    "categories": [
        "Home & Kitchen",
        "Kitchen & Dining",
        "Kitchen Utensils & Gadgets",
        "Shears"
    ],
    "asin": "B07PZF3QS3",
    "images": [
        "https://m.media-amazon.com/images/I/41E7ALk+uXL._AC_SL1200_.jpg",
        "https://m.media-amazon.com/images/I/710B9HpzMPL._AC_SL1500_.jpg"
    ],
    "delivery": [
        "FREE delivery Friday, October 25 on orders shipped by Amazon over $35",
        "Or fastest Same-Day delivery Today 10 AM - 3 PM. Order within 4 hrs 46 mins"
    ]
}
```
#### ä»£ç ç¤ºä¾‹ï¼š

ä»¥ä¸‹æ˜¯ä¸€ä¸ª Python è„šæœ¬ï¼Œç”¨äºè§¦å‘äºšé©¬é€Šäº§å“æ•°æ®æ”¶é›†å¹¶å°†ç»“æœå­˜å‚¨ä¸º JSON æ–‡ä»¶ï¼š
```python
import json
import requests
import time


def trigger_datasets(api_token, dataset_id, datasets):
    headers = {
        "Authorization": f"Bearer {api_token}",
        "Content-Type": "application/json",
    }

    trigger_url = (
        f"https://api.brightdata.com/datasets/v3/trigger?dataset_id={dataset_id}"
    )

    # Sending API request to trigger dataset collection
    response = requests.post(trigger_url, headers=headers, data=json.dumps(datasets))

    if response.status_code == 200:
        print("Data collection triggered successfully!")
        snapshot_id = response.json().get("snapshot_id")
        return snapshot_id if snapshot_id else print("No snapshot ID returned.")
    else:
        print(f"Error: {response.status_code} - {response.text}")
        return None


def get_snapshot_data(api_token, snapshot_id):
    headers = {"Authorization": f"Bearer {api_token}"}
    snapshot_url = (
        f"https://api.brightdata.com/datasets/v3/snapshot/{snapshot_id}?format=json"
    )

    # Polling until the snapshot data is ready
    while True:
        time.sleep(10)
        response = requests.get(snapshot_url, headers=headers)

        if response.status_code == 200:
            return response.json()
        elif response.status_code == 202:
            print("Snapshot still processing... retrying.")
        else:
            print(f"Error: {response.status_code} - {response.text}")
            return None


def store_data(data, filename="amazon_products_data.json"):
    if data:
        with open(filename, "w") as file:
            json.dump(data, file, indent=4)
        print(f"Data saved in {filename}.")
    else:
        print("No data to store.")


if __name__ == "__main__":
    API_TOKEN = "YOUR_API_TOKEN"
    DATASET_ID = "gd_l7q7dkf244hwjntr0"

    datasets = [
        {
            "url": "https://www.amazon.com/Quencher-FlowState-Stainless-Insulated-Smoothie/dp/B0CRMZHDG8"
        },
        {
            "url": "https://www.amazon.com/KitchenAid-Protective-Dishwasher-Stainless-8-72-Inch/dp/B07PZF3QS3"
        },
        {
            "url": "https://www.amazon.com/TruSkin-Naturals-Vitamin-Topical-Hyaluronic/dp/B01M4MCUAF"
        },
    ]

    # Trigger dataset collection
    snapshot_id = trigger_datasets(API_TOKEN, DATASET_ID, datasets)

    if snapshot_id:
        # Retrieve the data once the snapshot is ready
        data = get_snapshot_data(API_TOKEN, snapshot_id)
        if data:
            store_data(data)
```
æ‚¨å¯ä»¥é€šè¿‡ä¸‹è½½ [æ­¤ç¤ºä¾‹ JSON æ–‡ä»¶](https://github.com/bright-cn/Amazon-scraper/blob/main/output_data/amazon_products_data.json) æŸ¥çœ‹å®Œæ•´çš„è¾“å‡ºã€‚

### äºšé©¬é€Šè¯„è®ºæ•°æ®

é€šè¿‡æä¾›äº§å“ URL å’Œç‰¹å®šå‚æ•°ï¼ˆå¦‚æ—¶é—´èŒƒå›´ã€å…³é”®è¯ä»¥åŠè¦çˆ¬å–çš„è¯„è®ºæ•°é‡ï¼‰ï¼Œæ”¶é›†äºšé©¬é€Šçš„äº§å“è¯„è®ºã€‚

<img width="700" alt="bright-cn-web-scraper-api-amazon-product-reviews" src="https://github.com/bright-cn/Amazon-scraper/blob/main/images/bright-data-web-scraper-api-amazon-product-reviews.png">

#### å…³é”®è¾“å…¥å‚æ•°ï¼š
| **å‚æ•°**            | **ç±»å‹**   | **æè¿°**                                                                          | **æ˜¯å¦å¿…éœ€** |
|---------------------|------------|-----------------------------------------------------------------------------------|--------------|
| `url`               | `string`   | äºšé©¬é€Šäº§å“ URLï¼Œç”¨äºçˆ¬å–è¯„è®ºã€‚                                                     | æ˜¯           |
| `days_range`        | `number`   | çˆ¬å–è¯„è®ºæ—¶è€ƒè™‘çš„è¿‡å»å¤©æ•°ï¼ˆç•™ç©ºè¡¨ç¤ºæ— æ—¶é—´é™åˆ¶ï¼‰ã€‚                                   | å¦           |
| `keyword`           | `string`   | æ ¹æ®ç‰¹å®šå…³é”®è¯è¿‡æ»¤è¯„è®ºã€‚                                                           | å¦           |
| `num_of_reviews`    | `number`   | è¦çˆ¬å–çš„è¯„è®ºæ•°é‡ï¼ˆå¦‚æœæœªæä¾›ï¼Œå°†çˆ¬å–æ‰€æœ‰å¯ç”¨è¯„è®ºï¼‰ã€‚                                | å¦           |

#### æ€§èƒ½ï¼š
- æ¯ä¸ªè¾“å…¥çš„å¹³å‡å“åº”æ—¶é—´ï¼š1 åˆ† 1 ç§’

#### ç¤ºä¾‹è¾“å‡ºæ•°æ®ï¼š
ä»¥ä¸‹æ˜¯çˆ¬å–äºšé©¬é€Šè¯„è®ºåç”Ÿæˆçš„è¾“å‡ºç¤ºä¾‹ï¼š
```json
{
    "url": "https://www.amazon.com/RORSOU-R10-Headphones-Microphone-Lightweight/dp/B094NC89P9/",
    "product_name": "RORSOU R10 On-Ear Headphones with Microphone...",
    "product_rating": 4.5,
    "product_rating_object": {
        "one_star": 386,
        "two_star": 237,
        "three_star": 584,
        "four_star": 1493,
        "five_star": 7630
    },
    "rating": 5,
    "author_name": "Amazon Customer",
    "review_header": "Great Sound For the Price!",
    "review_text": "I bought these headphones twice...",
    "badge": "Verified Purchase",
    "review_posted_date": "September 7, 2024",
    "helpful_count": 3
}
```
#### ä»£ç ç¤ºä¾‹ï¼š
ä»¥ä¸‹æ˜¯ä¸€æ®µ Python è„šæœ¬ï¼Œç”¨äºè§¦å‘äºšé©¬é€Šè¯„è®ºæ•°æ®çš„æ”¶é›†ï¼Œå¹¶å°†ç»“æœå­˜å‚¨ä¸º JSON æ–‡ä»¶ï¼š
```python
import json
import requests
import time


def trigger_datasets(api_token, dataset_id, datasets):
    headers = {
        "Authorization": f"Bearer {api_token}",
        "Content-Type": "application/json",
    }

    trigger_url = (
        f"https://api.brightdata.com/datasets/v3/trigger?dataset_id={dataset_id}"
    )

    # Sending API request to trigger dataset collection
    response = requests.post(trigger_url, headers=headers, data=json.dumps(datasets))

    if response.status_code == 200:
        print("Data collection triggered successfully!")
        snapshot_id = response.json().get("snapshot_id")
        return snapshot_id if snapshot_id else print("No snapshot ID returned.")
    else:
        print(f"Error: {response.status_code} - {response.text}")
        return None


def get_snapshot_data(api_token, snapshot_id):
    headers = {"Authorization": f"Bearer {api_token}"}
    snapshot_url = (
        f"https://api.brightdata.com/datasets/v3/snapshot/{snapshot_id}?format=json"
    )

    # Polling until the snapshot data is ready
    while True:
        time.sleep(10)
        response = requests.get(snapshot_url, headers=headers)

        if response.status_code == 200:
            return response.json()
        elif response.status_code == 202:
            print("Snapshot still processing... retrying.")
        else:
            print(f"Error: {response.status_code} - {response.text}")
            return None


def store_data(data, filename="amazon_reviews_data.json"):
    if data:
        with open(filename, "w") as file:
            json.dump(data, file, indent=4)
        print(f"Data saved in {filename}.")
    else:
        print("No data to store.")


if __name__ == "__main__":
    API_TOKEN = "YOUR_API_TOKEN"
    DATASET_ID = "gd_le8e811kzy4ggddlq"

    datasets = [
        {
            "url": "https://www.amazon.com/RORSOU-R10-Headphones-Microphone-Lightweight/dp/B094NC89P9/",
            "days_range": 0,
            "num_of_reviews": 4,
            "keyword": "great",
        },
        {
            "url": "https://www.amazon.com/Solar-Eclipse-Glasses-Certified-Viewing/dp/B08GB3QC1H",
            "days_range": 0,
            "num_of_reviews": 4,
            "keyword": "",
        },
    ]

    # Trigger dataset collection
    snapshot_id = trigger_datasets(API_TOKEN, DATASET_ID, datasets)

    if snapshot_id:
        # Retrieve the data once the snapshot is ready
        data = get_snapshot_data(API_TOKEN, snapshot_id)
        if data:
            store_data(data)
```
æ‚¨å¯ä»¥é€šè¿‡ä¸‹è½½ [æ­¤ç¤ºä¾‹ JSON æ–‡ä»¶](https://github.com/bright-cn/Amazon-scraper/blob/main/output_data/amazon_reviews_data.json) æŸ¥çœ‹å®Œæ•´çš„è¾“å‡ºã€‚

### äºšé©¬é€Šäº§å“æœç´¢

é€šè¿‡æä¾›å…³é”®è¯è¿›è¡Œäºšé©¬é€Šäº§å“çš„æœç´¢ã€‚

<img width="700" alt="bright-cn-web-scraper-api-keyword-search" src="https://github.com/bright-cn/Amazon-scraper/blob/main/images/bright-data-web-scraper-api-keyword-search.png">

#### å…³é”®è¾“å…¥å‚æ•°ï¼š
| **å‚æ•°**           | **ç±»å‹**   | **æè¿°**                                      | **æ˜¯å¦å¿…éœ€** |
|-------------------|-----------|----------------------------------------------|--------------|
| `keyword`         | `string`  | ç”¨äºæœç´¢äº§å“çš„å…³é”®è¯ã€‚                         | æ˜¯           |
| `url`             | `string`  | è¦æœç´¢çš„åŸŸå URLã€‚                             | æ˜¯           |
| `pages_to_search` | `number`  | è¦æœç´¢çš„é¡µé¢æ•°é‡ã€‚                             | å¦           |

#### æ€§èƒ½ï¼š
- æ¯ä¸ªè¾“å…¥çš„å¹³å‡å“åº”æ—¶é—´ï¼š1 ç§’

#### ç¤ºä¾‹è¾“å‡ºæ•°æ®ï¼š
ä»¥ä¸‹æ˜¯é€šè¿‡å…³é”®è¯åœ¨äºšé©¬é€Šä¸Šæœç´¢äº§å“åç”Ÿæˆçš„è¾“å‡ºç¤ºä¾‹ï¼š
```json
{
    "asin": "B08H75RTZ8",
    "url": "https://www.amazon.com/Microsoft-Xbox-Gaming-Console-video-game/dp/B08H75RTZ8/ref=sr_1_1",
    "name": "Xbox Series X 1TB SSD Console - Includes Xbox Wireless Controller...",
    "sponsored": "false",
    "initial_price": 479,
    "final_price": 479,
    "currency": "USD",
    "sold": 2000,
    "rating": 4.8,
    "num_ratings": 28675,
    "variations": null,
    "badge": null,
    "brand": null,
    "delivery": ["FREE delivery"],
    "keyword": "X-box",
    "image": "https://m.media-amazon.com/images/I/616klipzdtL._AC_UY218_.jpg",
    "domain": "https://www.amazon.com/",
    "bought_past_month": 2000,
    "page_number": 1,
    "rank_on_page": 1,
    "timestamp": "2024-10-20T10:39:37.679Z",
    "input": {
        "keyword": "X-box",
        "url": "https://www.amazon.com",
        "pages_to_search": 1
    }
}
```
#### ä»£ç ç¤ºä¾‹ï¼š  
ä»¥ä¸‹æ˜¯ä¸€æ®µ Python è„šæœ¬ï¼Œç”¨äºåŸºäºå…³é”®è¯è§¦å‘äºšé©¬é€Šäº§å“æœç´¢ï¼Œå¹¶å°†ç»“æœå­˜å‚¨ä¸º JSON æ–‡ä»¶ï¼š
```python
import json
import requests
import time


def trigger_datasets(api_token, dataset_id, datasets):
    headers = {
        "Authorization": f"Bearer {api_token}",
        "Content-Type": "application/json",
    }

    trigger_url = (
        f"https://api.brightdata.com/datasets/v3/trigger?dataset_id={dataset_id}"
    )

    # Sending API request to trigger dataset collection
    response = requests.post(trigger_url, headers=headers, data=json.dumps(datasets))

    if response.status_code == 200:
        print("Data collection triggered successfully!")
        snapshot_id = response.json().get("snapshot_id")
        return snapshot_id if snapshot_id else print("No snapshot ID returned.")
    else:
        print(f"Error: {response.status_code} - {response.text}")
        return None


def get_snapshot_data(api_token, snapshot_id):
    headers = {"Authorization": f"Bearer {api_token}"}
    snapshot_url = (
        f"https://api.brightdata.com/datasets/v3/snapshot/{snapshot_id}?format=json"
    )

    # Polling until the snapshot data is ready
    while True:
        response = requests.get(snapshot_url, headers=headers)

        if response.status_code == 200:
            return response.json()
        elif response.status_code == 202:
            print("Snapshot still processing... retrying.")
        else:
            print(f"Error: {response.status_code} - {response.text}")
            return None
        time.sleep(10)


def store_data(data, filename="amazon_keywords_data.json"):
    if data:
        with open(filename, "w") as file:
            json.dump(data, file, indent=4)
        print(f"Data saved in {filename}.")
    else:
        print("No data to store.")


if __name__ == "__main__":
    API_TOKEN = "YOUR_API_TOKEN"
    DATASET_ID = "gd_lwdb4vjm1ehb499uxs"

    datasets = [
        {"keyword": "X-box", "url": "https://www.amazon.com", "pages_to_search": 1},
        {"keyword": "PS5", "url": "https://www.amazon.de"},
        {
            "keyword": "car cleaning kit",
            "url": "https://www.amazon.es",
            "pages_to_search": 4,
        },
    ]

    # Trigger dataset collection
    snapshot_id = trigger_datasets(API_TOKEN, DATASET_ID, datasets)

    if snapshot_id:
        # Retrieve the data once the snapshot is ready
        data = get_snapshot_data(API_TOKEN, snapshot_id)
        if data:
            store_data(data)
```
æ‚¨å¯ä»¥é€šè¿‡ä¸‹è½½ [æ­¤ç¤ºä¾‹ JSON æ–‡ä»¶](https://github.com/bright-cn/Amazon-scraper/blob/main/output_data/amazon_keywords_data.json) æŸ¥çœ‹å®Œæ•´çš„è¾“å‡ºã€‚

### äºšé©¬é€Šå–å®¶ä¿¡æ¯

é€šè¿‡æä¾›ç‰¹å®šçš„å–å®¶ URLï¼Œè·å–äºšé©¬é€Šå–å®¶çš„è¯¦ç»†ä¿¡æ¯ã€‚

<img width="700" alt="bright-cn-web-scraper-api-seller-info" src="https://github.com/bright-cn/Amazon-scraper/blob/main/images/bright-data-web-scraper-api-seller-info.png">

#### å…³é”®è¾“å…¥å‚æ•°ï¼š
| **å‚æ•°**       | **ç±»å‹**   | **æè¿°**                          | **æ˜¯å¦å¿…éœ€** |
|---------------|-----------|------------------------------------|--------------|
| `url`         | `string`  | äºšé©¬é€Šå–å®¶ URL                    | æ˜¯           |

#### æ€§èƒ½ï¼š
- æ¯ä¸ªè¾“å…¥çš„å¹³å‡å“åº”æ—¶é—´ï¼š1 ç§’

#### ç¤ºä¾‹è¾“å‡ºæ•°æ®ï¼š
ä»¥ä¸‹æ˜¯çˆ¬å–äºšé©¬é€Šå–å®¶ä¿¡æ¯åç”Ÿæˆçš„è¾“å‡ºç¤ºä¾‹ï¼š
```json
{
    "input": {
        "url": "https://www.amazon.com/sp?seller=A33W53J5GVPZ8K"
    },
    "seller_id": "A33W53J5GVPZ8K",
    "seller_name": "Peckomatic",
    "description": "Peckomatic is committed to providing each customer with the highest standard of customer service.",
    "detailed_info": [
        {"title": "Business Name"},
        {"title": "Business Address"}
    ],
    "stars": "4.5 out of 5 stars",
    "feedbacks": [
        {
            "date": "By Kao y. on November 16, 2021.",
            "stars": "5 out of 5 stars",
            "text": "It say not to exceed 10lbs total but I did anyway. My bird was 8lbs + the 3lb box = 11lbs. Bird arrived in great condition."
        },
        {
            "date": "By JL on June 9, 2021.",
            "stars": "1 out of 5 stars",
            "text": "How this seller packages its items is not acceptable..."
        }
    ],
    "rating_positive": "89%",
    "feedbacks_percentages": {
        "star_5": "80%",
        "star_4": "9%",
        "star_3": "7%",
        "star_2": "0%",
        "star_1": "5%"
    },
    "products_link": "https://www.amazon.com/s?me=A33W53J5GVPZ8K",
    "buisness_name": "Francis Kunnumpurath",
    "buisness_address": "2612 State Route 80, Lafayette, NY, 13084, US",
    "rating_count_lifetime": 44,
    "country": "US"
}
```
#### ä»£ç ç¤ºä¾‹ï¼š  
ä»¥ä¸‹æ˜¯ä¸€æ®µ Python è„šæœ¬ï¼Œç”¨äºè§¦å‘äºšé©¬é€Šå–å®¶æ•°æ®çš„æ”¶é›†ï¼Œå¹¶å°†ç»“æœå­˜å‚¨ä¸º JSON æ–‡ä»¶ï¼š
```python
import json
import requests
import time


def trigger_datasets(api_token, dataset_id, datasets):
    headers = {
        "Authorization": f"Bearer {api_token}",
        "Content-Type": "application/json",
    }

    trigger_url = (
        f"https://api.brightdata.com/datasets/v3/trigger?dataset_id={dataset_id}"
    )

    # Sending API request to trigger dataset collection
    response = requests.post(trigger_url, headers=headers, data=json.dumps(datasets))

    if response.status_code == 200:
        print("Data collection triggered successfully!")
        snapshot_id = response.json().get("snapshot_id")
        return snapshot_id if snapshot_id else print("No snapshot ID returned.")
    else:
        print(f"Error: {response.status_code} - {response.text}")
        return None


def get_snapshot_data(api_token, snapshot_id):
    headers = {"Authorization": f"Bearer {api_token}"}
    snapshot_url = (
        f"https://api.brightdata.com/datasets/v3/snapshot/{snapshot_id}?format=json"
    )

    # Polling until the snapshot data is ready
    while True:
        response = requests.get(snapshot_url, headers=headers)

        if response.status_code == 200:
            return response.json()
        elif response.status_code == 202:
            print("Snapshot still processing... retrying.")
        else:
            print(f"Error: {response.status_code} - {response.text}")
            return None
        time.sleep(10)


def store_data(data, filename="amazon_seller_data.json"):
    if data:
        with open(filename, "w") as file:
            json.dump(data, file, indent=4)
        print(f"Data saved in {filename}.")
    else:
        print("No data to store.")


if __name__ == "__main__":
    API_TOKEN = "API_TOKEN"
    DATASET_ID = "gd_lhotzucw1etoe5iw1k"

    # Define the dataset with seller URLs
    datasets = [
        {"url": "https://www.amazon.com/sp?seller=A33W53J5GVPZ8K"},
        {"url": "https://www.amazon.com/sp?seller=A33YXLPENB0JBD"},
        {"url": "https://www.amazon.com/sp?seller=A33ZG27WW2U3E6"},
    ]

    # Trigger dataset collection
    snapshot_id = trigger_datasets(API_TOKEN, DATASET_ID, datasets)

    if snapshot_id:
        # Retrieve the data once the snapshot is ready
        data = get_snapshot_data(API_TOKEN, snapshot_id)
        if data:
            store_data(data)
```
æ‚¨å¯ä»¥é€šè¿‡ä¸‹è½½ [æ­¤ç¤ºä¾‹ JSON æ–‡ä»¶](https://github.com/bright-cn/Amazon-scraper/blob/main/output_data/amazon_seller_data.json) æŸ¥çœ‹å®Œæ•´çš„è¾“å‡ºã€‚

### äºšé©¬é€Šç•…é”€å•†å“

é€šè¿‡æä¾›ç•…é”€å•†å“ç±»åˆ«çš„ URLï¼Œå‘ç°äºšé©¬é€Šä¸Šçš„çƒ­é”€äº§å“ã€‚

<img width="700" alt="bright-cn-web-scraper-api-amazon-best-sellers" src="https://github.com/bright-cn/Amazon-scraper/blob/main/images/bright-data-web-scraper-api-amazon-best-sellers.png">

#### å…³é”®è¾“å…¥å‚æ•°ï¼š

| **å‚æ•°**         | **ç±»å‹**  | **æè¿°**                                     | **æ˜¯å¦å¿…éœ€** |
|-----------------|----------|---------------------------------------------|--------------|
| `category_url`  | `string` | è¦çˆ¬å–çš„ç•…é”€å•†å“ç±»åˆ« URL                     | æ˜¯           |

#### æ€§èƒ½ï¼š
- æ¯ä¸ªè¾“å…¥çš„å¹³å‡å“åº”æ—¶é—´ï¼š6 åˆ† 49 ç§’

#### ç¤ºä¾‹è¾“å‡ºæ•°æ®ï¼š
ä»¥ä¸‹æ˜¯çˆ¬å–äºšé©¬é€Šç•…é”€å•†å“æ•°æ®åç”Ÿæˆçš„è¾“å‡ºç¤ºä¾‹ï¼š
```json
{
    "title": "Amazon Basics Multipurpose Copy Printer Paper, 8.5\" x 11\", 1 Ream, 500 Sheets, White",
    "seller_name": "Amazon.com",
    "brand": "Amazon Basics",
    "initial_price": 9.99,
    "final_price": 7.41,
    "currency": "USD",
    "availability": "In Stock",
    "reviews_count": 178695,
    "rating": 4.8,
    "categories": [
        "Office Products",
        "Paper",
        "Copy & Multipurpose Paper"
    ],
    "asin": "B01FV0F8H8",
    "buybox_seller": "Amazon.com",
    "discount": "-26%",
    "root_bs_rank": 1,
    "url": "https://www.amazon.com/AmazonBasics-Multipurpose-Copy-Printer-Paper/dp/B01FV0F8H8?th=1&psc=1",
    "image_url": "https://m.media-amazon.com/images/I/81x0cTHWQJL._AC_SL1500_.jpg",
    "delivery": [
        "FREE delivery Friday, October 25",
        "Same-Day delivery Today 10 AM - 3 PM"
    ],
    "features": [
        "1 ream (500 sheets) of 8.5 x 11 white copier and printer paper",
        "Works with laser/inkjet printers, copiers, and fax machines",
        "Smooth 20lb weight paper for consistent ink and toner distribution"
    ],
    "bought_past_month": 100000,
    "root_bs_category": "Office Products",
    "bs_category": "Copy & Multipurpose Paper",
    "bs_rank": 1,
    "amazon_choice": true,
    "badge": "Amazon's Choice",
    "seller_url": "https://www.amazon.com/sp?ie=UTF8&seller=ATVPDKIKX0DER&asin=B01FV0F8H8",
    "timestamp": "2024-10-20T13:30:56.666Z"
}
```
#### ä»£ç ç¤ºä¾‹ï¼š  
ä»¥ä¸‹æ˜¯ä¸€æ®µ Python è„šæœ¬ï¼Œç”¨äºè§¦å‘äºšé©¬é€Šç•…é”€å•†å“æ•°æ®çš„æ”¶é›†ï¼Œå¹¶å°†ç»“æœå­˜å‚¨ä¸º JSON æ–‡ä»¶ï¼š
```python
import json
import requests
import time


def trigger_datasets(api_token, dataset_id, datasets):
    headers = {
        "Authorization": f"Bearer {api_token}",
        "Content-Type": "application/json",
    }

    trigger_url = f"https://api.brightdata.com/datasets/v3/trigger?dataset_id={dataset_id}&type=discover_new&discover_by=best_sellers_url&limit_per_input=3"

    # Sending API request to trigger dataset collection
    response = requests.post(trigger_url, headers=headers, data=json.dumps(datasets))

    if response.status_code == 200:
        print("Data collection triggered successfully!")
        snapshot_id = response.json().get("snapshot_id")
        return snapshot_id if snapshot_id else print("No snapshot ID returned.")
    else:
        print(f"Error: {response.status_code} - {response.text}")
        return None


def get_snapshot_data(api_token, snapshot_id):
    headers = {"Authorization": f"Bearer {api_token}"}
    snapshot_url = (
        f"https://api.brightdata.com/datasets/v3/snapshot/{snapshot_id}?format=json"
    )

    # Polling until the snapshot data is ready
    while True:
        time.sleep(10)
        response = requests.get(snapshot_url, headers=headers)

        if response.status_code == 200:
            return response.json()
        elif response.status_code == 202:
            print("Snapshot still processing... retrying.")
        else:
            print(f"Error: {response.status_code} - {response.text}")
            return None


def store_data(data, filename="amazon_bestsellers_data.json"):
    if data:
        with open(filename, "w") as file:
            json.dump(data, file, indent=4)
        print(f"Data saved in {filename}.")
    else:
        print("No data to store.")


if __name__ == "__main__":
    API_TOKEN = "YOUR_API_TOKEN"
    DATASET_ID = "gd_l7q7dkf244hwjntr0"

    datasets = [
        {
            "category_url": "https://www.amazon.com/gp/bestsellers/office-products/ref=pd_zg_ts_office-products"
        },
    ]

    # Trigger dataset collection
    snapshot_id = trigger_datasets(API_TOKEN, DATASET_ID, datasets)

    if snapshot_id:
        # Retrieve the data once the snapshot is ready
        data = get_snapshot_data(API_TOKEN, snapshot_id)
        if data:
            store_data(data)
```
æ‚¨å¯ä»¥é€šè¿‡ä¸‹è½½ [æ­¤ç¤ºä¾‹ JSON æ–‡ä»¶](https://github.com/bright-cn/Amazon-scraper/blob/main/output_data/amazon_bestsellers.json) æŸ¥çœ‹å®Œæ•´çš„è¾“å‡ºã€‚

### æŒ‰ç±»åˆ« URL çš„äºšé©¬é€Šäº§å“

é€šè¿‡æä¾›ç‰¹å®šç±»åˆ«çš„ URLï¼Œå‘ç°å¹¶æ”¶é›†äºšé©¬é€Šäº§å“æ•°æ®ã€‚å¯é€šè¿‡æ’åºé€‰é¡¹å’ŒåŸºäºä½ç½®çš„è¿‡æ»¤å™¨è‡ªå®šä¹‰æœç´¢ã€‚

<img width="700" alt="bright-cn-web-scraper-api-discover-by-category-url" src="https://github.com/bright-cn/Amazon-scraper/blob/main/images/bright-data-web-scraper-api-discover-by-category-url.png">

#### å…³é”®è¾“å…¥å‚æ•°ï¼š
| **å‚æ•°**       | **ç±»å‹**   | **æè¿°**                                      | **æ˜¯å¦å¿…éœ€** |
|---------------|-----------|----------------------------------------------|--------------|
| `url`         | `string`  | è¦çˆ¬å–äº§å“çš„ç±»åˆ« URL                          | æ˜¯           |
| `sort_by`     | `string`  | äº§å“ç»“æœçš„æ’åºæ ‡å‡†                            | å¦           |
| `zipcode`     | `string`  | åŸºäºä½ç½®çš„äº§å“ç»“æœé‚®æ”¿ç¼–ç                      | å¦           |

#### æ€§èƒ½ï¼š
- æ¯ä¸ªè¾“å…¥çš„å¹³å‡å“åº”æ—¶é—´ï¼š16 åˆ† 16 ç§’

#### ç¤ºä¾‹è¾“å‡ºæ•°æ®ï¼š
ä»¥ä¸‹æ˜¯çˆ¬å–ç‰¹å®šç±»åˆ«ä¸­çš„äº§å“åç”Ÿæˆçš„æ•°æ®ç¤ºä¾‹ï¼š
```json
{
    "title": "Quilted Makeup Bag Floral Makeup Bag Cotton Makeup Bag",
    "brand": "WYJ",
    "price": 9.99,
    "currency": "USD",
    "availability": "In Stock",
    "rating": 5,
    "reviews_count": 1,
    "categories": [
        "Beauty & Personal Care",
        "Cosmetic Bags"
    ],
    "asin": "B0DC3WX7RM",
    "seller_name": "yisenshangmaoyouxiangongsi",
    "number_of_sellers": 1,
    "url": "https://www.amazon.com/WYJ-Quilted-Coquette-Aesthetic-Blue/dp/B0DC3WX7RM",
    "image_url": "https://m.media-amazon.com/images/I/71SI04tB6QL._AC_SL1500_.jpg",
    "product_dimensions": "8.7\"L x 2.8\"W x 5.1\"H",
    "item_weight": "2.5 Ounces",
    "variations": [
        {
            "name": "Pink",
            "asin": "B0DC3RKYPF",
            "price": 9.99
        },
        {
            "name": "Blue",
            "asin": "B0DC3WX7RM",
            "price": 9.99
        },
        {
            "name": "Purple",
            "asin": "B0DC47CDDT",
            "price": 9.99
        }
    ],
    "badge": "#1 New Release",
    "top_review": "I love everything about this bag! It's made well and a good size. Super cute!"
}
```
#### ä»£ç ç¤ºä¾‹ï¼š  
ä»¥ä¸‹æ˜¯ä¸€æ®µ Python è„šæœ¬ï¼Œç”¨äºè§¦å‘ä»æŒ‡å®šç±»åˆ« URL æ”¶é›†äº§å“æ•°æ®ï¼Œå¹¶å°†å…¶å­˜å‚¨ä¸º JSON æ–‡ä»¶ï¼š
```python
import json
import requests
import time


def trigger_datasets(api_token, dataset_id, datasets):
    headers = {
        "Authorization": f"Bearer {api_token}",
        "Content-Type": "application/json",
    }

    trigger_url = f"https://api.brightdata.com/datasets/v3/trigger?dataset_id={dataset_id}&type=discover_new&discover_by=category_url&limit_per_input=4"

    # Sending API request to trigger dataset collection
    response = requests.post(trigger_url, headers=headers, data=json.dumps(datasets))

    if response.status_code == 200:
        print("Data collection triggered successfully!")
        snapshot_id = response.json().get("snapshot_id")
        return snapshot_id if snapshot_id else print("No snapshot ID returned.")
    else:
        print(f"Error: {response.status_code} - {response.text}")
        return None


def get_snapshot_data(api_token, snapshot_id):
    headers = {"Authorization": f"Bearer {api_token}"}
    snapshot_url = (
        f"https://api.brightdata.com/datasets/v3/snapshot/{snapshot_id}?format=json"
    )

    # Polling until the snapshot data is ready
    while True:
        response = requests.get(snapshot_url, headers=headers)

        if response.status_code == 200:
            return response.json()
        elif response.status_code == 202:
            print("Snapshot still processing... retrying.")
        else:
            print(f"Error: {response.status_code} - {response.text}")
            return None
        time.sleep(10)


def store_data(data, filename="amazon_bestsellers_data.json"):
    if data:
        with open(filename, "w") as file:
            json.dump(data, file, indent=4)
        print(f"Data saved in {filename}.")
    else:
        print("No data to store.")


if __name__ == "__main__":
    API_TOKEN = "YOUR_API_TOKEN"
    DATASET_ID = "gd_l7q7dkf244hwjntr0"

    datasets = [
        {
            "url": "https://www.amazon.com/s?i=luggage-intl-ship",
            "sort_by": "Best Sellers",
            "zipcode": "10001",
        },
        {
            "url": "https://www.amazon.com/s?i=baby-products-intl-ship",
            "sort_by": "Avg. Customer Review",
            "zipcode": "",
        },
        {
            "url": "https://www.amazon.com/s?rh=n%3A16225012011&fs=true&ref=lp_16225012011_sar",
            "sort_by": "Price: Low to High",
            "zipcode": "",
        },
    ]

    # Trigger dataset collection
    snapshot_id = trigger_datasets(API_TOKEN, DATASET_ID, datasets)

    if snapshot_id:
        # Retrieve the data once the snapshot is ready
        data = get_snapshot_data(API_TOKEN, snapshot_id)
        if data:
            store_data(data)
```
æ‚¨å¯ä»¥é€šè¿‡ä¸‹è½½ [æ­¤ç¤ºä¾‹ JSON æ–‡ä»¶](https://github.com/bright-cn/Amazon-scraper/blob/main/output_data/amazon_discover_by_category_url.json) æŸ¥çœ‹å®Œæ•´çš„è¾“å‡ºã€‚

### æŒ‰å…³é”®è¯çš„äºšé©¬é€Šäº§å“

é€šè¿‡ä½¿ç”¨ç‰¹å®šå…³é”®è¯å‘ç°äº§å“ã€‚

<img width="700" alt="bright-cn-web-scraper-api-discover-by-keyword" src="https://github.com/bright-cn/Amazon-scraper/blob/main/images/bright-data-web-scraper-api-discover-by-keyword.png">

#### å…³é”®è¾“å…¥å‚æ•°ï¼š
| **å‚æ•°**       | **ç±»å‹**   | **æè¿°**                          | **æ˜¯å¦å¿…éœ€** |
|---------------|-----------|-----------------------------------|--------------|
| `keyword`     | `string`  | ç”¨äºæœç´¢äº§å“çš„å…³é”®è¯               | æ˜¯           |

#### æ€§èƒ½ï¼š
- æ¯ä¸ªè¾“å…¥çš„å¹³å‡å“åº”æ—¶é—´ï¼š2 åˆ† 46 ç§’

#### ç¤ºä¾‹è¾“å‡ºæ•°æ®ï¼š
ä»¥ä¸‹æ˜¯ä½¿ç”¨å…³é”®è¯æœç´¢äº§å“åç”Ÿæˆçš„è¾“å‡ºç¤ºä¾‹ï¼š
```json
{
    "title": "SYLVANIA ECO LED Light Bulb, A19 60W Equivalent, 750 Lumens, 2700K, Non-Dimmable, Frosted, Soft White - 8 Count (Pack of 1)",
    "brand": "LEDVANCE",
    "seller_name": "Amazon.com",
    "initial_price": 13.99,
    "final_price": 12.12,
    "currency": "USD",
    "discount": "-13%",
    "rating": 4.7,
    "reviews_count": 48418,
    "availability": "In Stock",
    "url": "https://www.amazon.com/Sylvania-40821-Equivalent-Efficient-Temperature/dp/B08FRSS4BF",
    "image_url": "https://m.media-amazon.com/images/I/81wKhRO66oL._AC_SL1500_.jpg",
    "delivery": [
        "FREE delivery Friday, October 25 on orders shipped by Amazon over $35",
        "Or Prime members get FREE delivery Tomorrow, October 21. Order within 8 hrs 8 mins. Join Prime"
    ],
    "features": [
        "60W Incandescent Replacement Bulb - 750 Lumens",
        "Long-lasting â€“ 7 years lifespan",
        "Energy-saving â€“ Estimated energy cost of $1.08 per year"
    ],
    "discovery_input": {
        "keyword": "light bulb"
    },
    "input": {
        "url": "https://www.amazon.com/Sylvania-40821-Equivalent-Efficient-Temperature/dp/B08FRSS4BF"
    }
}
```
#### ä»£ç ç¤ºä¾‹ï¼š  
ä»¥ä¸‹æ˜¯ä¸€æ®µ Python è„šæœ¬ï¼Œç”¨äºåŸºäºå…³é”®è¯è§¦å‘äºšé©¬é€Šäº§å“æ•°æ®çš„æ”¶é›†ï¼Œå¹¶å°†ç»“æœå­˜å‚¨ä¸º JSON æ–‡ä»¶ï¼š
```python
import json
import requests
import time


def trigger_datasets(
    api_token, dataset_id, datasets, dataset_type="discover_new", discover_by="keyword"
):
    headers = {
        "Authorization": f"Bearer {api_token}",
        "Content-Type": "application/json",
    }

    trigger_url = f"https://api.brightdata.com/datasets/v3/trigger?dataset_id={dataset_id}&type={dataset_type}&discover_by={discover_by}"

    # Sending API request to trigger dataset collection
    response = requests.post(trigger_url, headers=headers, data=json.dumps(datasets))

    if response.status_code == 200:
        print("Data collection triggered successfully!")
        snapshot_id = response.json().get("snapshot_id")
        return snapshot_id if snapshot_id else print("No snapshot ID returned.")
    else:
        print(f"Error: {response.status_code} - {response.text}")
        return None


def get_snapshot_data(api_token, snapshot_id):
    headers = {"Authorization": f"Bearer {api_token}"}
    snapshot_url = (
        f"https://api.brightdata.com/datasets/v3/snapshot/{snapshot_id}?format=json"
    )

    # Polling until the snapshot data is ready
    while True:
        response = requests.get(snapshot_url, headers=headers)

        if response.status_code == 200:
            return response.json()
        elif response.status_code == 202:
            print("Snapshot still processing... retrying.")
        else:
            print(f"Error: {response.status_code} - {response.text}")
            return None
        time.sleep(10)


def store_data(data, filename="amazon_keyword_data.json"):
    if data:
        with open(filename, "w") as file:
            json.dump(data, file, indent=4)
        print(f"Data saved in {filename}.")
    else:
        print("No data to store.")


if __name__ == "__main__":
    API_TOKEN = "API_TOKEN"
    DATASET_ID = "gd_l7q7dkf244hwjntr0"

    # Define the dataset with keywords
    datasets = [{"keyword": "light bulb"}, {"keyword": "dog toys"}]

    # Trigger dataset collection
    snapshot_id = trigger_datasets(API_TOKEN, DATASET_ID, datasets)

    if snapshot_id:
        # Retrieve the data once the snapshot is ready
        data = get_snapshot_data(API_TOKEN, snapshot_id)
        if data:
            store_data(data)
```
æ‚¨å¯ä»¥é€šè¿‡ä¸‹è½½ [æ­¤ç¤ºä¾‹ JSON æ–‡ä»¶](https://github.com/bright-cn/Amazon-scraper/blob/main/output_data/amazon_keyword_data.json) æŸ¥çœ‹å®Œæ•´çš„è¾“å‡ºã€‚

### å…¨çƒäºšé©¬é€Šäº§å“æ•°æ®é›†

é€šè¿‡æä¾› URLï¼Œä»æ‰€æœ‰ä¸»è¦äºšé©¬é€ŠåŸŸåä¸­æ”¶é›†äº§å“æ•°æ®ã€‚

<img width="700" alt="bright-cn-web-scraper-api-amazon-product-global-dataset" src="https://github.com/bright-cn/Amazon-scraper/blob/main/images/bright-data-web-scraper-api-amazon-product-global-dataset.png">

#### å…³é”®è¾“å…¥å‚æ•°ï¼š
| **å‚æ•°**       | **ç±»å‹**   | **æè¿°**                | **æ˜¯å¦å¿…éœ€** |
|---------------|-----------|------------------------|--------------|
| `url`         | `string`  | äºšé©¬é€Šäº§å“ URL          | æ˜¯           |

#### æ€§èƒ½ï¼š
- **æ¯ä¸ªè¾“å…¥çš„å¹³å‡å“åº”æ—¶é—´ï¼š** å°‘äº 1 ç§’

#### ç¤ºä¾‹è¾“å‡ºæ•°æ®ï¼š
ä»¥ä¸‹æ˜¯æ”¶é›†äº§å“æ•°æ®åç”Ÿæˆçš„è¾“å‡ºç¤ºä¾‹ï¼š
```json
{
    "title": "Toys of Wood Oxford Wooden Stacking Rings â€“ Learning to Count â€“ Counting Game with 45 Rings â€“ Wooden Toy for Ages 3 and Above",
    "brand": "Toys of Wood Oxford",
    "seller_name": "Toys of Wood Oxford",
    "initial_price": 23.99,
    "currency": "EUR",
    "final_price": 23.99,
    "availability": "Only 20 left in stock.",
    "rating": 4.5,
    "reviews_count": 1677,
    "asin": "B078TNNZK3",
    "url": "https://www.amazon.de/dp/B078TNNZK3?th=1&psc=1",
    "image_url": "https://m.media-amazon.com/images/I/815t1-d+7BL._AC_SL1500_.jpg",
    "product_dimensions": "43.31 x 11.61 x 11.51 cm; 830 g",
    "categories": [
        "Toys",
        "Baby & Toddler Toys",
        "Early Development & Activity Toys",
        "Sorting, Stacking & Plugging Toys"
    ],
    "delivery": [
        "FREE delivery Friday, 25 October on eligible first order",
        "Or fastest delivery Thursday, 24 October. Order within 4 hrs 40 mins"
    ],
    "features": [
        "Sturdy and stable base plate with 9 pins and 45 beautiful large wooden rings and 10 removable square number plates in rainbow colours.",
        "Great for learning counting, sorting, and matching colors and numbers, as well as practicing simple mathematics.",
        "Made from sustainable wood with eco-friendly and non-toxic paints. Complies with EN71 / CPSA standards."
    ],
    "top_review": "Sehr lehrreich",
    "variations": [
        {
            "name": "Caterpillar Threading Toy",
            "price": 13.99,
            "currency": "EUR"
        },
        {
            "name": "Pack of 15",
            "price": 16.99,
            "currency": "EUR"
        },
        {
            "name": "Pack of 45",
            "price": 23.99,
            "currency": "EUR"
        }
    ],
    "product_rating_object": {
        "one_star": 35,
        "two_star": 0,
        "three_star": 82,
        "four_star": 227,
        "five_star": 1308
    }
}
```
#### ä»£ç ç¤ºä¾‹ï¼š  
ä»¥ä¸‹æ˜¯ä¸€æ®µ Python è„šæœ¬ï¼Œç”¨äºè§¦å‘ä»æ‰€æœ‰ä¸»è¦äºšé©¬é€ŠåŸŸåæ”¶é›†äº§å“æ•°æ®ï¼Œå¹¶å°†ç»“æœå­˜å‚¨ä¸º JSON æ–‡ä»¶ï¼š
```python
import json
import requests
import time


def trigger_datasets(
    api_token, dataset_id, datasets, dataset_type="trigger", discover_by="url"
):
    headers = {
        "Authorization": f"Bearer {api_token}",
        "Content-Type": "application/json",
    }

    trigger_url = f"https://api.brightdata.com/datasets/v3/trigger?dataset_id={
        dataset_id}&type={dataset_type}&discover_by={discover_by}"

    # Sending API request to trigger dataset collection
    response = requests.post(trigger_url, headers=headers, data=json.dumps(datasets))

    if response.status_code == 200:
        print("Data collection triggered successfully!")
        snapshot_id = response.json().get("snapshot_id")
        return snapshot_id if snapshot_id else print("No snapshot ID returned.")
    else:
        print(f"Error: {response.status_code} - {response.text}")
        return None


def get_snapshot_data(api_token, snapshot_id):
    headers = {"Authorization": f"Bearer {api_token}"}
    snapshot_url = f"https://api.brightdata.com/datasets/v3/snapshot/{
        snapshot_id}?format=json"

    # Polling until the snapshot data is ready
    while True:
        response = requests.get(snapshot_url, headers=headers)

        if response.status_code == 200:
            return response.json()
        elif response.status_code == 202:
            print("Snapshot still processing... retrying.")
        else:
            print(f"Error: {response.status_code} - {response.text}")
            return None
        time.sleep(10)


def store_data(data, filename="amazon_products_global_dataset.json"):
    if data:
        with open(filename, "w") as file:
            json.dump(data, file, indent=4)
        print(f"Data saved in {filename}.")
    else:
        print("No data to store.")


if __name__ == "__main__":
    API_TOKEN = "API_TOKEN"
    DATASET_ID = "gd_lwhideng15g8jg63s7"

    # Define the dataset with URLs
    datasets = [
        {"url": "https://www.amazon.com/dp/B0CHHSFMRL/"},
        {
            "url": "https://www.amazon.de/-/en/dp/B078TNNZK3/ref=sspa_dk_browse_2/?_encoding=UTF8&ie=UTF8&sp_csd=d2lkZ2V0TmFtZT1zcF9icm93c2VfdGhlbWF0aWM%3D&pd_rd_w=fHlOu&content-id=amzn1.sym.642a11a6-0e1e-47fa-93c2-5dc9d607a7a1&pf_rd_p=642a11a6-0e1e-47fa-93c2-5dc9d607a7a1&pf_rd_r=4JX920KFM8Q7PR83HJ7V&pd_rd_wg=K1OVN&pd_rd_r=be656f87-1a09-4144-b7cf-4e932d6a73c4&ref_=sspa_dk_browse&th=1"
        },
        {
            "url": "https://www.amazon.co.jp/X-TRAK-Folding-Bicycle-Carbon-Adjustable/dp/B0CWV9YTLV/ref=sr_1_1_sspa?crid=3MKZ2ALHSLFOM&dib=eyJ2IjoiMSJ9.YnBVPwJ7nLxlNGHktwDTFM5v2evnsXlnZTJHJKuG8dLeeRCILpy0Knr3ofiKpUGQYi6xR6y4tgdtal85DJ8u6DD_n9r1oVCXdVo0NFmNAfStU6E-MhBig5p_gZGjluAYv5HgUIoEPl0v3iMiRxZNRfivqB-utxOkPOOfXIBHLemry17XcltUDTQqtJv-kP-ZqdP29mjD2cRlbkALtHPKU44MvBC9WUrNcUHAMrlAxtTAByuriywMqz-w2P0HCeehcZTJ1EiLf2VR8cxCiwuaUbIOU3tr1kDN6D7yYPrgRn4.6AOdSmJsksZkqLg8kNM6EvWxIFOijCsP2zo5NLHn1P4&dib_tag=se&keywords=Bicycles&qid=1716973495&sprefix=%2Caps%2C851&sr=8-1-spons&sp_csd=d2lkZ2V0TmFtZT1zcF9hdGY&psc=1"
        },
        {
            "url": "https://www.amazon.in/Watches-Women%EF%BC%8CLadies-Stainless-Waterproof-Luminous/dp/B0D31HBWG1/ref=sr_1_2_sspa?dib=eyJ2IjoiMSJ9.1zFa2vTCZdD-bv6Knt_pWqvcRZPSSTPDwgMClRJNsWqdyGdCmryjEAfWpd-ZhwhC3vvNx9A0G2Gt1R952e7huzlukge2bmJETNf-kHBoWS5kV6g0pUVapEyDOEAGcw5ZvWlkeuLQ9oIwuhckRC6ARCt2yglYV-1HpP7lVGXotK6K6tjrdKxUSAOZJSXeOGP3dGuYPTjo9sllOrwA7FC2GG00aDcsSTzURENFj1c2rS-vNHkYmxOL1JYuwDWK2PJdMpsmkJw3jeMdgaiw7jG5ppMfAjwiETVldQzhHGVUFV8.manfNZwtTUhvDuSGdh32APM1_SmnNiKgOGabyA7rXBo&dib_tag=se&qid=1716973272&rnid=2563505031&s=watch&sr=1-2-spons&sp_csd=d2lkZ2V0TmFtZT1zcF9hdGZfYnJvd3Nl&psc=1"
        },
    ]

    # Trigger dataset collection
    snapshot_id = trigger_datasets(API_TOKEN, DATASET_ID, datasets)

    if snapshot_id:
        # Retrieve the data once the snapshot is ready
        data = get_snapshot_data(API_TOKEN, snapshot_id)
        if data:
            store_data(data)
```
æ‚¨å¯ä»¥é€šè¿‡ä¸‹è½½ [æ­¤ç¤ºä¾‹ JSON æ–‡ä»¶](https://github.com/bright-cn/Amazon-scraper/blob/main/output_data/amazon_products_global_dataset.json) æŸ¥çœ‹å®Œæ•´çš„è¾“å‡ºã€‚

### å…¨çƒäºšé©¬é€Šäº§å“æ•°æ®é›† - æŒ‰ç±»åˆ« URL å‘ç°

é€šè¿‡æä¾›ç‰¹å®šçš„ç±»åˆ« URLï¼Œå‘ç°äº§å“æ•°æ®ã€‚

<img width="700" alt="bright-cn-web-scraper-api-amazon-product-global-category-url" src="https://github.com/bright-cn/Amazon-scraper/blob/main/images/bright-data-web-scraper-api-amazon-product-global-category-url.png">

#### å…³é”®è¾“å…¥å‚æ•°ï¼š
| **å‚æ•°**       | **ç±»å‹**   | **æè¿°**                                       | **æ˜¯å¦å¿…éœ€** |
|---------------|-----------|-----------------------------------------------|--------------|
| `url`         | `string`  | è¦çˆ¬å–äº§å“çš„ç±»åˆ« URL                            | æ˜¯           |
| `sort_by`     | `string`  | ç»“æœçš„æ’åºæ ‡å‡†                                  | å¦           |
| `zipcode`     | `string`  | åŸºäºä½ç½®çš„ç»“æœçš„é‚®æ”¿ç¼–ç                          | å¦           |

#### æ€§èƒ½ï¼š
- æ¯ä¸ªè¾“å…¥çš„å¹³å‡å“åº”æ—¶é—´ï¼š3 åˆ† 57 ç§’

#### ç¤ºä¾‹è¾“å‡ºæ•°æ®ï¼š
ä»¥ä¸‹æ˜¯æ”¶é›†äº§å“æ•°æ®åç”Ÿæˆçš„è¾“å‡ºç¤ºä¾‹ï¼š
```json
{
    "title": "De'Longhi Stilosa EC230.BK, Traditional Barista Pump Espresso Machine, Espresso and Cappuccino, 2 cups, Black",
    "brand": "De'Longhi",
    "seller_name": "Hughes Electrical",
    "initial_price": 104.99,
    "final_price": 94,
    "currency": "GBP",
    "availability": "Only 1 left in stock.",
    "rating": 3.9,
    "reviews_count": 395,
    "asin": "B085J8LV4F",
    "url": "https://www.amazon.co.uk/dp/B085J8LV4F?th=1&psc=1",
    "image_url": "https://m.media-amazon.com/images/I/715gqhkOEiL._AC_SL1500_.jpg",
    "categories": [
        "Cooking & Dining",
        "Coffee, Tea & Espresso",
        "Coffee Machines",
        "Espresso & Cappuccino Machines"
    ],
    "delivery": [
        "FREE delivery 25 - 28 October",
        "Or fastest delivery Tomorrow, 22 October. Order within 3 hrs 59 mins"
    ],
    "features": [
        "Unleash your inner barista and create all your coffee shop favourites at home",
        "15-bar pump espresso maker with a stainless steel boiler for perfect coffee extraction",
        "Steam arm to create frothy cappuccinos and smooth lattes",
        "Combination of matt and glossy black finish with an anti-drip system"
    ],
    "input": {
        "url": "https://www.amazon.co.uk/DeLonghi-EC230-BK-Traditional-Espresso-Cappuccino/dp/B085J8LV4F/ref=sr_1_4"
    },
    "discovery_input": {
        "url": "https://www.amazon.co.uk/b/?_encoding=UTF8&node=10706951&ref_=Oct_d_odnav_d_13528598031_1",
        "sort_by": "Best Sellers",
        "zipcode": ""
    }
}
```
#### ä»£ç ç¤ºä¾‹ï¼š  
ä»¥ä¸‹æ˜¯ä¸€æ®µ Python è„šæœ¬ï¼Œç”¨äºè§¦å‘æŒ‰ç±»åˆ« URL æ”¶é›†äº§å“æ•°æ®ï¼Œå¹¶å°†ç»“æœå­˜å‚¨ä¸º JSON æ–‡ä»¶ï¼š
```python
import json
import requests
import time


def trigger_datasets(api_token, dataset_id, datasets, dataset_type="discover_new", discover_by="category_url", limit_per_input=4):
    headers = {
        "Authorization": f"Bearer {api_token}",
        "Content-Type": "application/json",
    }

    trigger_url = f"https://api.brightdata.com/datasets/v3/trigger?dataset_id={dataset_id}&type={
        dataset_type}&discover_by={discover_by}&limit_per_input={limit_per_input}"

    # Sending API request to trigger dataset collection
    response = requests.post(
        trigger_url, headers=headers, data=json.dumps(datasets))

    if response.status_code == 200:
        print("Data collection triggered successfully!")
        snapshot_id = response.json().get("snapshot_id")
        return snapshot_id if snapshot_id else print("No snapshot ID returned.")
    else:
        print(f"Error: {response.status_code} - {response.text}")
        return None


def get_snapshot_data(api_token, snapshot_id):
    headers = {"Authorization": f"Bearer {api_token}"}
    snapshot_url = f"https://api.brightdata.com/datasets/v3/snapshot/{
        snapshot_id}?format=json"

    # Polling until the snapshot data is ready
    while True:
        response = requests.get(snapshot_url, headers=headers)

        if response.status_code == 200:
            return response.json()
        elif response.status_code == 202:
            print("Snapshot still processing... retrying.")
        else:
            print(f"Error: {response.status_code} - {response.text}")
            return None
        time.sleep(10)


def store_data(data, filename="amazon_category_url_data.json"):
    if data:
        with open(filename, "w") as file:
            json.dump(data, file, indent=4)
        print(f"Data saved in {filename}.")
    else:
        print("No data to store.")


if __name__ == "__main__":
    API_TOKEN = "API_TOKEN"
    DATASET_ID = "gd_lwhideng15g8jg63s7"

    # Define the dataset with category URLs, sort_by, and zipcodes
    datasets = [
        {"url": "https://www.amazon.com/s?i=luggage-intl-ship",
            "sort_by": "Featured", "zipcode": "10001"},
        {"url": "https://www.amazon.de/-/en/b/?node=1981001031&ref_=Oct_d_odnav_d_355007011_2&pd_rd_w=OjE3S&content-id=amzn1.sym.0069bc39-a323-47d6-a8fb-7558e4a563e4&pf_rd_p=0069bc39-a323-47d6-a8fb-7558e4a563e4&pf_rd_r=6YXZ7HGFNNEAF0GSDPDH&pd_rd_wg=0yR1G&pd_rd_r=a95cb46c-78ef-4b7b-845d-49fe04556440", "sort_by": "Price: Low to High", "zipcode": ""},
        {"url": "https://www.amazon.co.uk/b/?_encoding=UTF8&node=10706951&bbn=11052681&ref_=Oct_d_odnav_d_13528598031_1&pd_rd_w=LghVp&content-id=amzn1.sym.7414f21e-2c95-4394-9a75-8c1b3641bcea&pf_rd_p=7414f21e-2c95-4394-9a75-8c1b3641bcea&pf_rd_r=EE0PQWMSY2J0G8M032EB&pd_rd_wg=7snrU&pd_rd_r=349e1e79-8bf8-4e00-947d-17eab2942b8d", "sort_by": "Best Sellers", "zipcode": ""},
        {"url": "https://www.amazon.co.jp/-/en/b/?node=377403011&ref_=Oct_d_odnav_d_15314601_0&pd_rd_w=ajUV4&content-id=amzn1.sym.0d505cca-fde9-497c-b5f8-e827c26fad17&pf_rd_p=0d505cca-fde9-497c-b5f8-e827c26fad17&pf_rd_r=92HSETNKKN3RTA615BV7&pd_rd_wg=AwOOk&pd_rd_r=629211d8-6768-478c-94a2-829a0a0ca2a6", "sort_by": "", "zipcode": ""}
    ]

    # Trigger dataset collection
    snapshot_id = trigger_datasets(API_TOKEN, DATASET_ID, datasets)

    if snapshot_id:
        # Retrieve the data once the snapshot is ready
        data = get_snapshot_data(API_TOKEN, snapshot_id)
        if data:
            store_data(data)
```
æ‚¨å¯ä»¥é€šè¿‡ä¸‹è½½ [æ­¤ç¤ºä¾‹ JSON æ–‡ä»¶](https://github.com/bright-cn/Amazon-scraper/blob/main/output_data/amazon_product_global_category_url.json) æŸ¥çœ‹å®Œæ•´çš„è¾“å‡ºã€‚

### å…¨çƒäºšé©¬é€Šäº§å“æ•°æ®é›† - æŒ‰å…³é”®è¯å‘ç°

é€šè¿‡ä½¿ç”¨ç‰¹å®šå…³é”®è¯ï¼Œåœ¨äºšé©¬é€Šå„ä¸ªåŸŸåä¸­å‘ç°äº§å“æ•°æ®ã€‚

<img width="700" alt="bright-cn-web-scraper-api-amazon_global_dataset_by_keyword" src="https://github.com/bright-cn/Amazon-scraper/blob/main/images/bright-data-web-scraper-api-amazon_global_dataset_by_keyword.png">

#### å…³é”®è¾“å…¥å‚æ•°ï¼š
| **å‚æ•°**           | **ç±»å‹**   | **æè¿°**                                   | **æ˜¯å¦å¿…éœ€** |
|--------------------|-----------|-------------------------------------------|--------------|
| `keywords`         | `string`  | ç”¨äºæœç´¢äº§å“çš„å…³é”®è¯                        | æ˜¯           |
| `domain`           | `string`  | è¦æœç´¢çš„äºšé©¬é€ŠåŸŸå                          | æ˜¯           |
| `pages_to_search`  | `number`  | è¦æœç´¢çš„é¡µé¢æ•°é‡                            | å¦           |

#### æ€§èƒ½ï¼š
- æ¯ä¸ªè¾“å…¥çš„å¹³å‡å“åº”æ—¶é—´ï¼š56 ç§’

#### ç¤ºä¾‹è¾“å‡ºæ•°æ®ï¼š
ä»¥ä¸‹æ˜¯é€šè¿‡å…³é”®è¯æœç´¢äº§å“åç”Ÿæˆçš„è¾“å‡ºç¤ºä¾‹ï¼š
```json
{
    "title": "Mitutoyo 500-197-30 Electronic Digital Caliper AOS Absolute Scale Digital Caliper, 0 to 8\"/0 to 200mm Measuring Range, 0.0005\"/0.01mm Resolution",
    "brand": "Mitutoyo",
    "seller_name": "Everly Home & Gift",
    "initial_price": 157.97,
    "final_price": 137.77,
    "currency": "USD",
    "availability": "In Stock",
    "rating": 4.8,
    "reviews_count": 88,
    "asin": "B01N6C3EGR",
    "url": "https://www.amazon.com/dp/B01N6C3EGR?th=1&psc=1",
    "image_url": "https://m.media-amazon.com/images/I/61Gigoh3LbL._SL1500_.jpg",
    "categories": [
        "Industrial & Scientific",
        "Test, Measure & Inspect",
        "Dimensional Measurement",
        "Calipers",
        "Digital Calipers"
    ],
    "delivery": [
        "FREE delivery Saturday, October 26",
        "Or Prime members get FREE delivery Tomorrow, October 22"
    ],
    "features": [
        "Hardened stainless steel construction for protection of caliper components",
        "Digital, single-value readout LCD display in metric units for readability",
        "Measuring Range 0 to 8\"/0 to 200mm",
        "Measurement Accuracy +/-0.001",
        "Resolution 0.0005\"/0.01mm"
    ],
    "input": {
        "url": "https://www.amazon.com/Mitutoyo-500-197-30-Electronic-Measuring-Resolution/dp/B01N6C3EGR"
    },
    "discovery_input": {
        "keywords": "Mitutoyo",
        "domain": "https://www.amazon.com",
        "pages_to_search": 1
    }
}
```
#### ä»£ç ç¤ºä¾‹ï¼š  
ä»¥ä¸‹æ˜¯ä¸€æ®µ Python è„šæœ¬ï¼Œç”¨äºè§¦å‘æŒ‰å…³é”®è¯æœç´¢äº§å“æ•°æ®çš„æ”¶é›†ï¼Œå¹¶å°†ç»“æœå­˜å‚¨ä¸º JSON æ–‡ä»¶ï¼š
```python
import json
import requests
import time


def trigger_datasets(
    api_token, dataset_id, datasets, dataset_type="discover_new", discover_by="keywords"
):
    headers = {
        "Authorization": f"Bearer {api_token}",
        "Content-Type": "application/json",
    }

    trigger_url = f"https://api.brightdata.com/datasets/v3/trigger?dataset_id={
        dataset_id}&type={dataset_type}&discover_by={discover_by}"

    # Sending API request to trigger dataset collection
    response = requests.post(trigger_url, headers=headers, data=json.dumps(datasets))

    if response.status_code == 200:
        print("Data collection triggered successfully!")
        snapshot_id = response.json().get("snapshot_id")
        return snapshot_id if snapshot_id else print("No snapshot ID returned.")
    else:
        print(f"Error: {response.status_code} - {response.text}")
        return None


def get_snapshot_data(api_token, snapshot_id):
    headers = {"Authorization": f"Bearer {api_token}"}
    snapshot_url = f"https://api.brightdata.com/datasets/v3/snapshot/{
        snapshot_id}?format=json"

    # Polling until the snapshot data is ready
    while True:
        response = requests.get(snapshot_url, headers=headers)

        if response.status_code == 200:
            return response.json()
        elif response.status_code == 202:
            print("Snapshot still processing... retrying.")
        else:
            print(f"Error: {response.status_code} - {response.text}")
            return None
        time.sleep(10)


def store_data(data, filename="amazon_global_dataset_by_keyword.json"):
    if data:
        with open(filename, "w") as file:
            json.dump(data, file, indent=4)
        print(f"Data saved in {filename}.")
    else:
        print("No data to store.")


if __name__ == "__main__":
    API_TOKEN = "YOUR_API_TOKEN"
    DATASET_ID = "gd_lwhideng15g8jg63s7"

    # Define the dataset with keywords, domain, and pages_to_search
    datasets = [
        {
            "keywords": "Mitutoyo",
            "domain": "https://www.amazon.com",
            "pages_to_search": 1,
        },
        {
            "keywords": "smart watch",
            "domain": "https://www.amazon.co.uk",
            "pages_to_search": 2,
        },
        {
            "keywords": "football",
            "domain": "https://www.amazon.in",
            "pages_to_search": 4,
        },
        {
            "keywords": "baby cloth",
            "domain": "https://www.amazon.de",
            "pages_to_search": 3,
        },
    ]

    # Trigger dataset collection
    snapshot_id = trigger_datasets(API_TOKEN, DATASET_ID, datasets)

    if snapshot_id:
        # Retrieve the data once the snapshot is ready
        data = get_snapshot_data(API_TOKEN, snapshot_id)
        if data:
            store_data(data)
```
æ‚¨å¯ä»¥é€šè¿‡ä¸‹è½½ [æ­¤ç¤ºä¾‹ JSON æ–‡ä»¶](https://github.com/bright-cn/Amazon-scraper/blob/main/output_data/amazon_global_dataset_by_keyword.json) æŸ¥çœ‹å®Œæ•´çš„è¾“å‡ºã€‚
